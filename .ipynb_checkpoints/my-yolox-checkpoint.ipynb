{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78df2c7",
   "metadata": {},
   "source": [
    " @author:QiuhaoXie\n",
    "\n",
    " @time:2022/2/19\n",
    "\n",
    " @title:yolox-s 复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92d1b9f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-22T11:15:52.598809Z",
     "start_time": "2022-02-22T11:15:51.166511Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aaf928",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "基础网络模块的复现：\n",
    "BaseConv，DWConv，Bottleneck，ResLayer，SPPBottleneck，CSPLayer，Focus\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15d9710f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-22T11:15:53.803721Z",
     "start_time": "2022-02-22T11:15:53.723984Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_activation(name=\"silu\", inplace=True):\n",
    "    if name == \"silu\":\n",
    "        module = nn.SiLU(inplace=inplace)\n",
    "    elif name == \"relu\":\n",
    "        module = nn.ReLU(inplace=inplace)\n",
    "    elif name == \"lrelu\":\n",
    "        module = nn.LeakyReLU(0.1, inplace=inplace)\n",
    "    else:\n",
    "        raise AttributeError(\"Unsupported act type: {}\".format(name))\n",
    "    return module\n",
    "\n",
    "\n",
    "class BaseConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, ksize, stride, groups=1, bias=False, act=\"silu\"):\n",
    "        super(BaseConv, self).__init__()\n",
    "        pad = (ksize - 1) // 2\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=ksize,\n",
    "            stride=stride,\n",
    "            padding=pad,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = get_activation(act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    "\n",
    "    def fuseforward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "\n",
    "class DWConv(nn.Module):\n",
    "    \"\"\" Depthwise Conv + Conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, ksize, stride=1, act=\"silu\"):\n",
    "        super(DWConv, self).__init__()\n",
    "        self.dconv = BaseConv(\n",
    "            in_channels,\n",
    "            in_channels,\n",
    "            ksize=ksize,\n",
    "            stride=stride,\n",
    "            groups=in_channels,\n",
    "            act=act\n",
    "        )\n",
    "        self.pconv = BaseConv(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            ksize=1,\n",
    "            stride=1,\n",
    "            groups=1,\n",
    "            act=act\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pconv(self.dconv(x))\n",
    "\n",
    "\n",
    "class Focus(nn.Module):\n",
    "    \"\"\"Focus width and height information into channel space.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, ksize=1, stride=1, act=\"silu\"):\n",
    "        super(Focus, self).__init__()\n",
    "        self.conv = BaseConv(in_channels * 4, out_channels, ksize, stride, act=act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape of x (b,c,w,h) -> y(b,4c,w/2,h/2)\n",
    "        patch_top_left = x[..., ::2, ::2]\n",
    "        patch_top_right = x[..., ::2, 1::2]\n",
    "        patch_bot_left = x[..., 1::2, ::2]\n",
    "        patch_bot_right = x[..., 1::2, 1::2]\n",
    "        x = torch.cat(\n",
    "            (\n",
    "                patch_top_left,\n",
    "                patch_bot_left,\n",
    "                patch_top_right,\n",
    "                patch_bot_right,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )  # 把图片进行剪接，拼接在深度，图片大小缩小为原来的1/2\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ResLayer(nn.Module):\n",
    "    \"\"\"Residual layer with `in_channels` inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int):\n",
    "        super(ResLayer, self).__init__()\n",
    "        mid_channels = in_channels // 2\n",
    "        self.layer1 = BaseConv(in_channels, mid_channels, ksize=1, stride=1, act=\"lrelu\")\n",
    "        self.layer2 = BaseConv(mid_channels\n",
    "                               , in_channels, ksize=3, stride=1, act=\"lrelu\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer2(self.layer1(x))\n",
    "        out += x  # 同纬度相加\n",
    "        return out  # 图片大小不变\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    \"\"\"Standard bottleneck 可对应Res unit\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, shortcut=True, expansion=0.5, depthwise=False, act=\"silu\"):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        Conv = DWConv if depthwise else BaseConv\n",
    "        self.conv1 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=act)\n",
    "        self.conv2 = Conv(hidden_channels, out_channels, 3, stride=1, act=act)\n",
    "        self.use_add = shortcut and in_channels == out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv2(self.conv1(x))\n",
    "        if self.use_add:\n",
    "            y += x\n",
    "        return y\n",
    "\n",
    "\n",
    "class CSPLayer(nn.Module):\n",
    "    \"\"\"C3 in yolov5, CSP Bottleneck with 3 convolutions,对应CSP1_x的网络\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, n=1, shortcut=True, expansion=0.5, depthwise=False, act=\"silu\"):\n",
    "        super(CSPLayer, self).__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): input channels.\n",
    "            out_channels (int): output channels.\n",
    "            n (int): number of Bottlenecks. Default value: 1.\n",
    "        \"\"\"\n",
    "        hidden_channels = int(out_channels * expansion)\n",
    "        self.conv1 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=act)\n",
    "        self.conv2 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=act)\n",
    "        self.conv3 = BaseConv(2 * hidden_channels, out_channels, 1, stride=1, act=act)\n",
    "        module_list = [\n",
    "            Bottleneck(hidden_channels, hidden_channels, shortcut, 1.0, depthwise, act=act)\n",
    "            for _ in range(n)\n",
    "        ]\n",
    "        self.m = nn.Sequential(*module_list)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_1 = self.conv1(x)\n",
    "        x_2 = self.conv2(x)\n",
    "        x_1 = self.m(x_1)\n",
    "        out = torch.cat((x_1, x_2), dim=1)\n",
    "        return self.conv3(out)\n",
    "\n",
    "\n",
    "class SPPBottleneck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes=(5, 9, 13), act=\"silu\"):\n",
    "        super(SPPBottleneck, self).__init__()\n",
    "        hidden_channels = in_channels // 2\n",
    "        self.conv1 = BaseConv(in_channels, hidden_channels, 1, stride=1, act=act)\n",
    "        self.m = nn.ModuleList(\n",
    "            [nn.MaxPool2d(kernel_size=ks, stride=1, padding=ks // 2) for ks in kernel_sizes]\n",
    "        )\n",
    "        conv2_channels = hidden_channels * (len(kernel_sizes) + 1)\n",
    "        self.conv2 = BaseConv(conv2_channels, out_channels, 1, stride=1, act=act)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        out = torch.cat([x] + [m(x) for m in self.m], dim=1)\n",
    "        return self.conv2(out)\n",
    "\n",
    "\n",
    "class CSPDarknet(nn.Module):\n",
    "    def __init__(self, dep_mul, wid_mul, out_features=(\"dark3\", \"dark4\", \"dark5\"), depthwise=False, act=\"silu\"):\n",
    "        super(CSPDarknet, self).__init__()\n",
    "        assert out_features, \"please provide output features of Darknet\"\n",
    "        self.out_features = out_features\n",
    "        Conv = DWConv if depthwise else BaseConv\n",
    "        base_channels = int(wid_mul * 64)  # 64\n",
    "        base_depth = max(round(dep_mul * 3), 1)  # 3\n",
    "\n",
    "        # stem\n",
    "        self.stem = Focus(3, base_channels, ksize=3, act=act)\n",
    "\n",
    "        # dark2\n",
    "        self.dark2 = nn.Sequential(Conv(base_channels, base_channels * 2, 3, 2, act=act),\n",
    "                                   CSPLayer(base_channels * 2, base_channels * 2, n=base_depth, depthwise=depthwise,\n",
    "                                            act=act), )\n",
    "\n",
    "        # dark3\n",
    "        self.dark3 = nn.Sequential(Conv(base_channels * 2, base_channels * 4, 3, 2, act=act),\n",
    "                                   CSPLayer(\n",
    "                                       base_channels * 4,\n",
    "                                       base_channels * 4,\n",
    "                                       n=base_depth,\n",
    "                                       depthwise=depthwise,\n",
    "                                       act=act,\n",
    "                                   ), )\n",
    "\n",
    "        # dark4\n",
    "        self.dark4 = nn.Sequential(\n",
    "            Conv(base_channels * 4, base_channels * 8, 3, 2, act=act),\n",
    "            CSPLayer(\n",
    "                base_channels * 8,\n",
    "                base_channels * 8,\n",
    "                n=base_depth,\n",
    "                depthwise=depthwise,\n",
    "                act=act\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # dark5\n",
    "        self.dark5 = nn.Sequential(\n",
    "            Conv(base_channels * 8, base_channels * 16, 3, 2, act=act),\n",
    "            SPPBottleneck(base_channels * 16, base_channels * 16, act=act),\n",
    "            CSPLayer(\n",
    "                base_channels * 16,\n",
    "                base_channels * 16,\n",
    "                n=base_depth,\n",
    "                shortcut=False,\n",
    "                depthwise=depthwise,\n",
    "                act=act,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = {}\n",
    "        x = self.stem(x)\n",
    "        outputs[\"stem\"] = x\n",
    "        x = self.dark2(x)\n",
    "        outputs[\"dark2\"] = x\n",
    "        x = self.dark3(x)\n",
    "        outputs[\"dark3\"] = x\n",
    "        x = self.dark4(x)\n",
    "        outputs[\"dark4\"] = x\n",
    "        x = self.dark5(x)\n",
    "        outputs[\"dark5\"] = x\n",
    "        return {k: v for k, v in outputs.items() if k in self.out_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd9a04a",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "YOLOx-s损失函数的复现：IOUloss\n",
    "boxes的复现：bboxes_iou\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdfab726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-22T11:15:59.834043Z",
     "start_time": "2022-02-22T11:15:59.815103Z"
    }
   },
   "outputs": [],
   "source": [
    "class IOUloss(nn.Module):\n",
    "    def __init__(self, reduction=\"none\", loss_type=\"iou\"):\n",
    "        super(IOUloss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        assert pred.shape[0] == target.shape[0]\n",
    "\n",
    "        pred = pred.view(-1, 4)\n",
    "        target = target.view(-1, 4)\n",
    "        tl = torch.max(\n",
    "            (pred[:, :2] - pred[:, 2:] / 2), (target[:, :2] - target[:, 2:] / 2)\n",
    "        )\n",
    "        br = torch.min(\n",
    "            (pred[:, :2] + pred[:, 2:] / 2), (target[:, :2] + target[:, 2:] / 2)\n",
    "        )\n",
    "\n",
    "        area_p = torch.prod(pred[:, 2:], 1)\n",
    "        area_g = torch.prod(target[:, 2:], 1)\n",
    "\n",
    "        en = (tl < br).type(tl.type()).prod(dim=1)\n",
    "        area_i = torch.prod(br - tl, 1) * en\n",
    "        area_u = area_p + area_g - area_i\n",
    "        iou = (area_i) / (area_u + 1e-16)\n",
    "\n",
    "        if self.loss_type == \"iou\":\n",
    "            loss = 1 - iou ** 2\n",
    "        elif self.loss_type == \"giou\":\n",
    "            c_tl = torch.min(\n",
    "                (pred[:, :2] - pred[:, 2:] / 2), (target[:, :2] - target[:, 2:] / 2)\n",
    "            )\n",
    "            c_br = torch.max(\n",
    "                (pred[:, :2] + pred[:, 2:] / 2), (target[:, :2] + target[:, 2:] / 2)\n",
    "            )\n",
    "            area_c = torch.prod(c_br - c_tl, 1)\n",
    "            giou = iou - (area_c - area_u) / area_c.clamp(1e-16)\n",
    "            loss = 1 - giou.clamp(min=-1.0, max=1.0)\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "def bboxes_iou(bboxes_a, bboxes_b, xyxy=True):\n",
    "    if bboxes_a.shape[1] != 4 or bboxes_b.shape[1] != 4:\n",
    "        raise IndexError\n",
    "\n",
    "    if xyxy:\n",
    "        tl = torch.max(bboxes_a[:, None, :2], bboxes_b[:, :2])\n",
    "        br = torch.min(bboxes_a[:, None, 2:], bboxes_b[:, 2:])\n",
    "        area_a = torch.prod(bboxes_a[:, 2:] - bboxes_a[:, :2], 1)\n",
    "        area_b = torch.prod(bboxes_b[:, 2:] - bboxes_b[:, :2], 1)\n",
    "    else:\n",
    "        tl = torch.max(\n",
    "            (bboxes_a[:, None, :2] - bboxes_a[:, None, 2:] / 2),\n",
    "            (bboxes_b[:, :2] - bboxes_b[:, 2:] / 2),\n",
    "        )\n",
    "        br = torch.min(\n",
    "            (bboxes_a[:, None, :2] + bboxes_a[:, None, 2:] / 2),\n",
    "            (bboxes_b[:, :2] + bboxes_b[:, 2:] / 2),\n",
    "        )\n",
    "\n",
    "        area_a = torch.prod(bboxes_a[:, 2:], 1)\n",
    "        area_b = torch.prod(bboxes_b[:, 2:], 1)\n",
    "    en = (tl < br).type(tl.type()).prod(dim=2)\n",
    "    area_i = torch.prod(br - tl, 2) * en  # * ((tl < br).all())\n",
    "    return area_i / (area_a[:, None] + area_b - area_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65855b5e",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "YOLOX-s的复现：Backbone,FPN Neck,Prediction(解耦头)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4392d26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-22T11:16:03.055491Z",
     "start_time": "2022-02-22T11:16:02.799358Z"
    },
    "code_folding": [
     0,
     101
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "class YOLOPAFPN(nn.Module):\n",
    "    \"\"\"\n",
    "    此处Backbone ,FPN Neck 的复现\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            depth=1.0,\n",
    "            width=1.0,\n",
    "            in_features=(\"dark3\", \"dark4\", \"dark5\"),\n",
    "            in_channels=[256, 512, 1024],\n",
    "            depthwise=False,\n",
    "            act=\"silu\",\n",
    "    ):\n",
    "        super(YOLOPAFPN, self).__init__()\n",
    "        print(depth)\n",
    "        self.backbone = CSPDarknet(dep_mul=depth, wid_mul=width, depthwise=depthwise, act=act)\n",
    "        self.in_features = in_features\n",
    "        self.in_channels = in_channels\n",
    "        Conv = DWConv if depthwise else BaseConv\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        self.lateral_conv0 = BaseConv(\n",
    "            int(in_channels[2] * width), int(in_channels[1] * width), 1, 1, act=act\n",
    "        )\n",
    "        self.C3_p4 = CSPLayer(\n",
    "            int(2 * in_channels[1] * width),\n",
    "            int(in_channels[1] * width),\n",
    "            round(3 * depth),\n",
    "            False,\n",
    "            depthwise=depthwise,\n",
    "            act=act,\n",
    "        )\n",
    "\n",
    "        self.reduce_conv1 = BaseConv(\n",
    "            int(in_channels[1] * width), int(in_channels[0] * width), 1, 1, act=act,\n",
    "        )\n",
    "\n",
    "        self.C3_p3 = CSPLayer(\n",
    "            int(in_channels[0] * width * 2), int(in_channels[0] * width), round(3 * depth), False, depthwise=depthwise,\n",
    "            act=act\n",
    "        )\n",
    "\n",
    "        # bottom-up conv\n",
    "        self.bu_conv2 = Conv(\n",
    "            int(in_channels[0] * width), int(in_channels[0] * width), 3, 2, act=act\n",
    "        )\n",
    "        self.C3_n3 = CSPLayer(\n",
    "            int(2 * in_channels[0] * width),\n",
    "            int(in_channels[1] * width),\n",
    "            round(3 * depth),\n",
    "            False,\n",
    "            depthwise=depthwise,\n",
    "            act=act,\n",
    "        )\n",
    "\n",
    "        # bottom-up conv\n",
    "        self.bu_conv1 = Conv(int(in_channels[1] * width), int(in_channels[1] * width), 3, 2, act=act)\n",
    "        self.C3_n4 = CSPLayer(\n",
    "            int(in_channels[1] * width * 2),\n",
    "            int(in_channels[2] * width),\n",
    "            round(3 * depth),\n",
    "            False,\n",
    "            depthwise=depthwise,\n",
    "            act=act,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x:input images.(tensor)\n",
    "        Returns:\n",
    "            Tuple[Tensor]:FPN feature.\n",
    "        \"\"\"\n",
    "        # backbone\n",
    "        out_features = self.backbone(x)\n",
    "        features = [out_features[f] for f in self.in_features]\n",
    "        [x2, x1, x0] = features  # dark3,dark4,dark5(channel:4,8,16,size:/channel)\n",
    "\n",
    "        fpn_out0 = self.lateral_conv0(x0)  # 1024->512/32\n",
    "        f_out0 = self.upsample(fpn_out0)  # 512/16\n",
    "        f_out0 = torch.cat([f_out0, x1], dim=1)  # 512->1024/16\n",
    "        f_out0 = self.C3_p4(f_out0)  # 1024->512/16\n",
    "\n",
    "        fpn_out1 = self.reduce_conv1(f_out0)  # 512->256/16\n",
    "        f_out1 = self.upsample(fpn_out1)  # 256/8\n",
    "        f_out1 = torch.cat([f_out1, x2], dim=1)  # 256->512/8\n",
    "        pan_out2 = self.C3_p3(f_out1)  # 512->256/8\n",
    "\n",
    "        p_out1 = self.bu_conv2(pan_out2)  # 256->256/16\n",
    "        p_out1 = torch.cat([p_out1, fpn_out1], 1)  # 256->512/16\n",
    "        pan_out1 = self.C3_n3(p_out1)  # 512->512/16\n",
    "\n",
    "        p_out0 = self.bu_conv1(pan_out1)  # 512->512/32\n",
    "        p_out0 = torch.cat([p_out0, fpn_out0], 1)  # 512->1024/32\n",
    "        pan_out0 = self.C3_n4(p_out0)  # 1024->1024/32\n",
    "\n",
    "        outputs = (pan_out2, pan_out1, pan_out0)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class YOLOXHead(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_classes,\n",
    "            width=1.0,\n",
    "            strides=[8, 16, 32],\n",
    "            in_channels=[256, 512, 1024],\n",
    "            act=\"silu\",\n",
    "            depthwise=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            act (str): activation type of conv. Defalut value: \"silu\".\n",
    "            depthwise (bool): whether apply depthwise conv in conv branch. Defalut value: False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_anchors = 1  # 把图片分为n份，其中每个框预测一个框\n",
    "        self.num_classes = num_classes\n",
    "        self.decode_in_inference = True  # for deploy, set to False\n",
    "\n",
    "        self.cls_convs = nn.ModuleList()  # 因为有多尺度所以使用nn.ModuleList(),cls表示class：预测的总类别数\n",
    "        self.reg_convs = nn.ModuleList()\n",
    "        self.cls_preds = nn.ModuleList()\n",
    "        self.reg_preds = nn.ModuleList()\n",
    "        self.obj_preds = nn.ModuleList()\n",
    "        self.stems = nn.ModuleList()\n",
    "        Conv = DWConv if depthwise else BaseConv\n",
    "\n",
    "        for i in range(len(in_channels)):\n",
    "            self.stems.append(\n",
    "                BaseConv(\n",
    "                    in_channels=int(in_channels[i] * width),\n",
    "                    out_channels=int(256 * width),\n",
    "                    ksize=1,\n",
    "                    stride=1,\n",
    "                    act=act,\n",
    "                )\n",
    "            )\n",
    "            self.cls_convs.append(\n",
    "                nn.Sequential(\n",
    "                    *[\n",
    "                        Conv(\n",
    "                            in_channels=int(256 * width),\n",
    "                            out_channels=int(256 * width),\n",
    "                            ksize=3,\n",
    "                            stride=1,\n",
    "                            act=act,\n",
    "                        ),\n",
    "                        Conv(\n",
    "                            in_channels=int(256 * width),\n",
    "                            out_channels=int(256 * width),\n",
    "                            ksize=3,\n",
    "                            stride=1,\n",
    "                            act=act,\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            self.reg_convs.append(\n",
    "                nn.Sequential(\n",
    "                    *[\n",
    "                        Conv(\n",
    "                            in_channels=int(256 * width),\n",
    "                            out_channels=int(256 * width),\n",
    "                            ksize=3,\n",
    "                            stride=1,\n",
    "                            act=act,\n",
    "                        ),\n",
    "                        Conv(\n",
    "                            in_channels=int(256 * width),\n",
    "                            out_channels=int(256 * width),\n",
    "                            ksize=3,\n",
    "                            stride=1,\n",
    "                            act=act,\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            self.cls_preds.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=int(256 * width),\n",
    "                    out_channels=self.n_anchors * self.num_classes,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                )\n",
    "            )\n",
    "            self.reg_preds.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=int(256 * width),\n",
    "                    out_channels=4,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                )\n",
    "            )\n",
    "            self.obj_preds.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=int(256 * width),\n",
    "                    out_channels=self.n_anchors * 1,\n",
    "                    kernel_size=1,\n",
    "                    stride=1,\n",
    "                    padding=0,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.use_l1 = False\n",
    "        self.l1_loss = nn.L1Loss(reduction=\"none\")\n",
    "        self.bcewithlog_loss = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        self.iou_loss = IOUloss(reduction=\"none\")\n",
    "        self.strides = strides\n",
    "        self.grids = [torch.zeros(1)] * len(in_channels)\n",
    "\n",
    "    def initialize_biases(self, prior_prob):\n",
    "        for conv in self.cls_preds:\n",
    "            b = conv.bias.view(self.n_anchors, -1)\n",
    "            b.data.fill_(-math.log((1 - prior_prob) / prior_prob))\n",
    "            conv.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\n",
    "\n",
    "        for conv in self.obj_preds:\n",
    "            b = conv.bias.view(self.n_anchors, -1)\n",
    "            b.data.fill_(-math.log((1 - prior_prob) / prior_prob))\n",
    "            conv.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\n",
    "\n",
    "    def forward(self, xin, labels=None, imgs=None):\n",
    "        outputs = []\n",
    "        origin_preds = []\n",
    "        x_shifts = []\n",
    "        y_shifts = []\n",
    "        expanded_strides = []\n",
    "\n",
    "        for k, (cls_conv, reg_conv, stride_this_level, x) in enumerate(\n",
    "                zip(self.cls_convs, self.reg_convs, self.strides, xin)\n",
    "        ):\n",
    "            x = self.stems[k](x)\n",
    "            cls_x = x\n",
    "            reg_x = x\n",
    "\n",
    "            cls_feat = cls_conv(cls_x)\n",
    "            cls_output = self.cls_preds[k](cls_feat)  # [batch_num,total_class_num(总分类数),H,W]\n",
    "\n",
    "            reg_feat = reg_conv(reg_x)\n",
    "            reg_output = self.reg_preds[k](reg_feat)  # [batch_num,4(框的四边),H,W] H*W->格子数 一个格子预测一个框\n",
    "            obj_output = self.obj_preds[k](reg_feat)  # [batch_num,1(置信度),H,W]\n",
    "\n",
    "            if self.training:\n",
    "                output = torch.cat([reg_output, obj_output, cls_output], 1)\n",
    "                output, grid = self.get_output_and_grid(\n",
    "                    output, k, stride_this_level, xin[0].type()\n",
    "                )  # 得到每个格子的输出和左上角的坐标，reg_output 还原到原尺寸，从而便于与标签进行损失计算\n",
    "                x_shifts.append(grid[:, :, 0])  # 得到每个格子的x坐标\n",
    "                y_shifts.append(grid[:, :, 1])  # 得到每个格子的y坐标\n",
    "                expanded_strides.append(\n",
    "                    torch.zeros(1, grid.shape[1])\n",
    "                        .fill_(stride_this_level)\n",
    "                        .type_as(xin[0])\n",
    "                )  # 得到每个格子的步长\n",
    "                if self.use_l1:\n",
    "                    batch_size = reg_output.shape[0]\n",
    "                    hsize, wsize = reg_output.shape[-2:]\n",
    "                    reg_output = reg_output.view(\n",
    "                        batch_size, self.n_anchors, 4, hsize, wsize\n",
    "                    )\n",
    "                    reg_output = reg_output.permute(0, 1, 3, 4, 2).reshape(\n",
    "                        batch_size, -1, 4\n",
    "                    )\n",
    "                    origin_preds.append(reg_output.clone())\n",
    "\n",
    "            else:\n",
    "                output = torch.cat(\n",
    "                    [reg_output, obj_output.sigmoid(), cls_output.sigmoid()], 1\n",
    "                )\n",
    "\n",
    "            outputs.append(output)  # 共有三个规格80*80，40*40，20*20\n",
    "\n",
    "        if self.training:\n",
    "            return self.get_losses(\n",
    "                imgs,\n",
    "                x_shifts,\n",
    "                y_shifts,\n",
    "                expanded_strides,\n",
    "                labels,\n",
    "                torch.cat(outputs, 1),\n",
    "                origin_preds,\n",
    "                dtype=xin[0].dtype,\n",
    "            )\n",
    "        else:\n",
    "            self.hw = [x.shape[-2:] for x in outputs]\n",
    "            # [batch, n_anchors_all, 85]\n",
    "            outputs = torch.cat(\n",
    "                [x.flatten(start_dim=2) for x in outputs], dim=2\n",
    "            ).permute(0, 2, 1)\n",
    "            if self.decode_in_inference:\n",
    "                return self.decode_outputs(outputs, dtype=xin[0].type())\n",
    "            else:\n",
    "                return outputs\n",
    "\n",
    "    def get_output_and_grid(self, output, k, stride, dtype):\n",
    "        grid = self.grids[k]  # 记录格子左上角坐标\n",
    "\n",
    "        batch_size = output.shape[0]\n",
    "        n_ch = 5 + self.num_classes  # 此处表示记录预测值，4个边框预测，1个置信度\n",
    "        hsize, wsize = output.shape[-2:]\n",
    "        if grid.shape[2:4] != output.shape[2:4]:\n",
    "            yv, xv = torch.meshgrid([torch.arange(hsize), torch.arange(wsize)])  # 生成网格坐标\n",
    "            grid = torch.stack((xv, yv), 2).view(1, 1, hsize, wsize, 2).type(dtype)\n",
    "            self.grids[k] = grid\n",
    "\n",
    "        output = output.view(batch_size, self.n_anchors, n_ch, hsize, wsize)\n",
    "        output = output.permute(0, 1, 3, 4, 2).reshape(\n",
    "            batch_size, self.n_anchors * hsize * wsize, -1\n",
    "        )\n",
    "        grid = grid.view(1, -1, 2)\n",
    "        output[..., :2] = (output[..., :2] + grid) * stride  # （前两维+grid）*步长 ->预测左上角坐标\n",
    "        output[..., 2:4] = torch.exp(output[..., 2:4]) * stride\n",
    "        return output, grid\n",
    "\n",
    "    def decode_outputs(self, outputs, dtype):\n",
    "        grids = []\n",
    "        strides = []\n",
    "        for (hsize, wsize), stride in zip(self.hw, self.strides):\n",
    "            yv, xv = torch.meshgrid([torch.arange(hsize), torch.arange(wsize)])\n",
    "            grid = torch.stack((xv, yv), 2).view(1, -1, 2)\n",
    "            grids.append(grid)\n",
    "            shape = grid.shape[:2]\n",
    "            strides.append(torch.full((*shape, 1), stride))\n",
    "\n",
    "        grids = torch.cat(grids, dim=1).type(dtype)\n",
    "        strides = torch.cat(strides, dim=1).type(dtype)\n",
    "\n",
    "        outputs[..., :2] = (outputs[..., :2] + grids) * strides\n",
    "        outputs[..., 2:4] = torch.exp(outputs[..., 2:4]) * strides\n",
    "        return outputs\n",
    "\n",
    "    def get_losses(\n",
    "            self,\n",
    "            imgs,\n",
    "            x_shifts,\n",
    "            y_shifts,\n",
    "            expanded_strides,\n",
    "            labels,\n",
    "            outputs,\n",
    "            origin_preds,\n",
    "            dtype,\n",
    "    ):\n",
    "        bbox_preds = outputs[:, :, :4]  # [batch, n_anchors_all, 4]\n",
    "        obj_preds = outputs[:, :, 4].unsqueeze(-1)  # [batch, n_anchors_all, 1] 置信度预测\n",
    "        cls_preds = outputs[:, :, 5:]  # [batch, n_anchors_all, n_cls：预测的种类]\n",
    "\n",
    "        # calculate targets\n",
    "        nlabel = (labels.sum(dim=2) > 0).sum(\n",
    "            dim=1)  # number of objects,nlabel:每张图片有多少个目标框，labels->[batch,120(限定最多每张图片最多的目标框)，5(4个边框信息+1个分类值)]\n",
    "\n",
    "        total_num_anchors = outputs.shape[1]\n",
    "        x_shifts = torch.cat(x_shifts, 1)  # [1, n_anchors_all]\n",
    "        y_shifts = torch.cat(y_shifts, 1)  # [1, n_anchors_all]\n",
    "        expanded_strides = torch.cat(expanded_strides, 1)\n",
    "        if self.use_l1:\n",
    "            origin_preds = torch.cat(origin_preds, 1)\n",
    "\n",
    "        cls_targets = []\n",
    "        reg_targets = []\n",
    "        l1_targets = []\n",
    "        obj_targets = []\n",
    "        fg_masks = []\n",
    "\n",
    "        num_fg = 0.0\n",
    "        num_gts = 0.0\n",
    "\n",
    "        for batch_idx in range(outputs.shape[0]):\n",
    "            num_gt = int(nlabel[batch_idx])\n",
    "            num_gts += num_gt\n",
    "            if num_gt == 0:\n",
    "                cls_target = outputs.new_zeros((0, self.num_classes))\n",
    "                reg_target = outputs.new_zeros((0, 4))\n",
    "                l1_target = outputs.new_zeros((0, 4))\n",
    "                obj_target = outputs.new_zeros((total_num_anchors, 1))\n",
    "                fg_mask = outputs.new_zeros(total_num_anchors).bool()\n",
    "            else:\n",
    "                gt_bboxes_per_image = labels[batch_idx, :num_gt, 1:5]  # 每张图片的box\n",
    "                gt_classes = labels[batch_idx, :num_gt, 0]  # 真实分类\n",
    "                bboxes_preds_per_image = bbox_preds[batch_idx]  # 每张图片的预测框\n",
    "\n",
    "                try:\n",
    "                    (\n",
    "                        gt_matched_classes,\n",
    "                        fg_mask,\n",
    "                        pred_ious_this_matching,\n",
    "                        matched_gt_inds,\n",
    "                        num_fg_img,\n",
    "                    ) = self.get_assignments(  # noqa\n",
    "                        batch_idx,\n",
    "                        num_gt,\n",
    "                        total_num_anchors,\n",
    "                        gt_bboxes_per_image,\n",
    "                        gt_classes,\n",
    "                        bboxes_preds_per_image,\n",
    "                        expanded_strides,\n",
    "                        x_shifts,\n",
    "                        y_shifts,\n",
    "                        cls_preds,\n",
    "                        bbox_preds,\n",
    "                        obj_preds,\n",
    "                        labels,\n",
    "                        imgs,\n",
    "                    )\n",
    "                except RuntimeError:\n",
    "                    logger.error(\n",
    "                        \"OOM RuntimeError is raised due to the huge memory cost during label assignment. \\\n",
    "                           CPU mode is applied in this batch. If you want to avoid this issue, \\\n",
    "                           try to reduce the batch size or image size.\"\n",
    "                    )\n",
    "                    torch.cuda.empty_cache()\n",
    "                    (\n",
    "                        gt_matched_classes,\n",
    "                        fg_mask,\n",
    "                        pred_ious_this_matching,\n",
    "                        matched_gt_inds,\n",
    "                        num_fg_img,\n",
    "                    ) = self.get_assignments(  # noqa\n",
    "                        batch_idx,\n",
    "                        num_gt,\n",
    "                        total_num_anchors,\n",
    "                        gt_bboxes_per_image,\n",
    "                        gt_classes,\n",
    "                        bboxes_preds_per_image,\n",
    "                        expanded_strides,\n",
    "                        x_shifts,\n",
    "                        y_shifts,\n",
    "                        cls_preds,\n",
    "                        bbox_preds,\n",
    "                        obj_preds,\n",
    "                        labels,\n",
    "                        imgs,\n",
    "                        \"cpu\",\n",
    "                    )\n",
    "\n",
    "                torch.cuda.empty_cache()\n",
    "                num_fg += num_fg_img\n",
    "\n",
    "                cls_target = F.one_hot(\n",
    "                    gt_matched_classes.to(torch.int64), self.num_classes\n",
    "                ) * pred_ious_this_matching.unsqueeze(-1)\n",
    "                obj_target = fg_mask.unsqueeze(-1)\n",
    "                reg_target = gt_bboxes_per_image[matched_gt_inds]\n",
    "                if self.use_l1:\n",
    "                    l1_target = self.get_l1_target(\n",
    "                        outputs.new_zeros((num_fg_img, 4)),\n",
    "                        gt_bboxes_per_image[matched_gt_inds],\n",
    "                        expanded_strides[0][fg_mask],\n",
    "                        x_shifts=x_shifts[0][fg_mask],\n",
    "                        y_shifts=y_shifts[0][fg_mask],\n",
    "                    )\n",
    "\n",
    "            cls_targets.append(cls_target)\n",
    "            reg_targets.append(reg_target)\n",
    "            obj_targets.append(obj_target.to(dtype))\n",
    "            fg_masks.append(fg_mask)\n",
    "            if self.use_l1:\n",
    "                l1_targets.append(l1_target)\n",
    "\n",
    "        cls_targets = torch.cat(cls_targets, 0)\n",
    "        reg_targets = torch.cat(reg_targets, 0)\n",
    "        obj_targets = torch.cat(obj_targets, 0)\n",
    "        fg_masks = torch.cat(fg_masks, 0)\n",
    "        if self.use_l1:\n",
    "            l1_targets = torch.cat(l1_targets, 0)\n",
    "\n",
    "        num_fg = max(num_fg, 1)\n",
    "        loss_iou = (\n",
    "                       self.iou_loss(bbox_preds.view(-1, 4)[fg_masks], reg_targets)\n",
    "                   ).sum() / num_fg\n",
    "        loss_obj = (\n",
    "                       self.bcewithlog_loss(obj_preds.view(-1, 1), obj_targets)\n",
    "                   ).sum() / num_fg\n",
    "        loss_cls = (\n",
    "                       self.bcewithlog_loss(\n",
    "                           cls_preds.view(-1, self.num_classes)[fg_masks], cls_targets\n",
    "                       )\n",
    "                   ).sum() / num_fg\n",
    "        if self.use_l1:\n",
    "            loss_l1 = (\n",
    "                          self.l1_loss(origin_preds.view(-1, 4)[fg_masks], l1_targets)\n",
    "                      ).sum() / num_fg\n",
    "        else:\n",
    "            loss_l1 = 0.0\n",
    "\n",
    "        reg_weight = 5.0\n",
    "        loss = reg_weight * loss_iou + loss_obj + loss_cls + loss_l1\n",
    "\n",
    "        return (\n",
    "            loss,\n",
    "            reg_weight * loss_iou,\n",
    "            loss_obj,\n",
    "            loss_cls,\n",
    "            loss_l1,\n",
    "            num_fg / max(num_gts, 1),\n",
    "        )\n",
    "\n",
    "    def get_l1_target(self, l1_target, gt, stride, x_shifts, y_shifts, eps=1e-8):\n",
    "        l1_target[:, 0] = gt[:, 0] / stride - x_shifts\n",
    "        l1_target[:, 1] = gt[:, 1] / stride - y_shifts\n",
    "        l1_target[:, 2] = torch.log(gt[:, 2] / stride + eps)\n",
    "        l1_target[:, 3] = torch.log(gt[:, 3] / stride + eps)\n",
    "        return l1_target\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def get_assignments(\n",
    "            self,\n",
    "            batch_idx,  # 每次取batch中的一张图片的idx\n",
    "            num_gt,  # 真实框（gt）\n",
    "            total_num_anchors,  # 预测框8400个\n",
    "            gt_bboxes_per_image,\n",
    "            gt_classes,\n",
    "            bboxes_preds_per_image,  # 8400个预测框\n",
    "            expanded_strides,\n",
    "            x_shifts,\n",
    "            y_shifts,\n",
    "            cls_preds,  # 8400个分类\n",
    "            bbox_preds,\n",
    "            obj_preds,  # 8400个置信度\n",
    "            labels,\n",
    "            imgs,\n",
    "            mode=\"gpu\",\n",
    "    ):\n",
    "\n",
    "        if mode == \"cpu\":\n",
    "            print(\"------------CPU Mode for This Batch-------------\")\n",
    "            gt_bboxes_per_image = gt_bboxes_per_image.cpu().float()\n",
    "            bboxes_preds_per_image = bboxes_preds_per_image.cpu().float()\n",
    "            gt_classes = gt_classes.cpu().float()\n",
    "            expanded_strides = expanded_strides.cpu().float()\n",
    "            x_shifts = x_shifts.cpu()\n",
    "            y_shifts = y_shifts.cpu()\n",
    "\n",
    "        fg_mask, is_in_boxes_and_center = self.get_in_boxes_info(\n",
    "            gt_bboxes_per_image,\n",
    "            expanded_strides,\n",
    "            x_shifts,\n",
    "            y_shifts,\n",
    "            total_num_anchors,\n",
    "            num_gt,\n",
    "        )\n",
    "\n",
    "        bboxes_preds_per_image = bboxes_preds_per_image[fg_mask]\n",
    "        cls_preds_ = cls_preds[batch_idx][fg_mask]\n",
    "        obj_preds_ = obj_preds[batch_idx][fg_mask]\n",
    "        num_in_boxes_anchor = bboxes_preds_per_image.shape[0]\n",
    "\n",
    "        if mode == \"cpu\":\n",
    "            gt_bboxes_per_image = gt_bboxes_per_image.cpu()\n",
    "            bboxes_preds_per_image = bboxes_preds_per_image.cpu()\n",
    "\n",
    "        pair_wise_ious = bboxes_iou(gt_bboxes_per_image, bboxes_preds_per_image, False)\n",
    "\n",
    "        gt_cls_per_image = (\n",
    "            F.one_hot(gt_classes.to(torch.int64), self.num_classes)\n",
    "                .float()\n",
    "                .unsqueeze(1)\n",
    "                .repeat(1, num_in_boxes_anchor, 1)\n",
    "        )\n",
    "        pair_wise_ious_loss = -torch.log(pair_wise_ious + 1e-8)\n",
    "\n",
    "        if mode == \"cpu\":\n",
    "            cls_preds_, obj_preds_ = cls_preds_.cpu(), obj_preds_.cpu()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=False):\n",
    "            cls_preds_ = (\n",
    "                    cls_preds_.float().unsqueeze(0).repeat(num_gt, 1, 1).sigmoid_()\n",
    "                    * obj_preds_.float().unsqueeze(0).repeat(num_gt, 1, 1).sigmoid_()\n",
    "            )\n",
    "            pair_wise_cls_loss = F.binary_cross_entropy(\n",
    "                cls_preds_.sqrt_(), gt_cls_per_image, reduction=\"none\"\n",
    "            ).sum(-1)\n",
    "        del cls_preds_\n",
    "\n",
    "        cost = (\n",
    "                pair_wise_cls_loss\n",
    "                + 3.0 * pair_wise_ious_loss\n",
    "                + 100000.0 * (~is_in_boxes_and_center)\n",
    "        )\n",
    "\n",
    "        (\n",
    "            num_fg,\n",
    "            gt_matched_classes,\n",
    "            pred_ious_this_matching,\n",
    "            matched_gt_inds,\n",
    "        ) = self.dynamic_k_matching(cost, pair_wise_ious, gt_classes, num_gt, fg_mask)\n",
    "        del pair_wise_cls_loss, cost, pair_wise_ious, pair_wise_ious_loss\n",
    "\n",
    "        if mode == \"cpu\":\n",
    "            gt_matched_classes = gt_matched_classes.cuda()\n",
    "            fg_mask = fg_mask.cuda()\n",
    "            pred_ious_this_matching = pred_ious_this_matching.cuda()\n",
    "            matched_gt_inds = matched_gt_inds.cuda()\n",
    "\n",
    "        return (\n",
    "            gt_matched_classes,\n",
    "            fg_mask,\n",
    "            pred_ious_this_matching,\n",
    "            matched_gt_inds,\n",
    "            num_fg,\n",
    "        )\n",
    "\n",
    "    def get_in_boxes_info(\n",
    "            self,\n",
    "            gt_bboxes_per_image,\n",
    "            expanded_strides,\n",
    "            x_shifts,\n",
    "            y_shifts,\n",
    "            total_num_anchors,\n",
    "            num_gt,\n",
    "    ):\n",
    "        expanded_strides_per_image = expanded_strides[0]\n",
    "        x_shifts_per_image = x_shifts[0] * expanded_strides_per_image\n",
    "        y_shifts_per_image = y_shifts[0] * expanded_strides_per_image  # 左上角坐标\n",
    "        x_centers_per_image = (\n",
    "            (x_shifts_per_image + 0.5 * expanded_strides_per_image)  # 每个格子中心坐标\n",
    "                .unsqueeze(0)\n",
    "                .repeat(num_gt, 1)  # 复制num_gt个进行预测框比较\n",
    "        )  # [n_anchor] -> [n_gt, n_anchor]\n",
    "        y_centers_per_image = (\n",
    "            (y_shifts_per_image + 0.5 * expanded_strides_per_image)\n",
    "                .unsqueeze(0)\n",
    "                .repeat(num_gt, 1)\n",
    "        )\n",
    "        # 计算真实框的四边，计算左上角，右下角坐标\n",
    "        gt_bboxes_per_image_l = (\n",
    "            (gt_bboxes_per_image[:, 0] - 0.5 * gt_bboxes_per_image[:, 2])\n",
    "                .unsqueeze(1)\n",
    "                .repeat(1, total_num_anchors)\n",
    "        )\n",
    "        gt_bboxes_per_image_r = (\n",
    "            (gt_bboxes_per_image[:, 0] + 0.5 * gt_bboxes_per_image[:, 2])\n",
    "                .unsqueeze(1)\n",
    "                .repeat(1, total_num_anchors)\n",
    "        )  # 此处是中心点加上长和宽\n",
    "        gt_bboxes_per_image_t = (\n",
    "            (gt_bboxes_per_image[:, 1] - 0.5 * gt_bboxes_per_image[:, 3])\n",
    "                .unsqueeze(1)\n",
    "                .repeat(1, total_num_anchors)\n",
    "        )\n",
    "        gt_bboxes_per_image_b = (\n",
    "            (gt_bboxes_per_image[:, 1] + 0.5 * gt_bboxes_per_image[:, 3])\n",
    "                .unsqueeze(1)\n",
    "                .repeat(1, total_num_anchors)\n",
    "        )\n",
    "\n",
    "        b_l = x_centers_per_image - gt_bboxes_per_image_l\n",
    "        b_r = gt_bboxes_per_image_r - x_centers_per_image\n",
    "        b_t = y_centers_per_image - gt_bboxes_per_image_t\n",
    "        b_b = gt_bboxes_per_image_b - y_centers_per_image\n",
    "        bbox_deltas = torch.stack([b_l, b_t, b_r, b_b], 2)\n",
    "\n",
    "        is_in_boxes = bbox_deltas.min(dim=-1).values > 0.0  # bbox_deltas->[num_gt:真实框数,8400：3种规格的总格子数]\n",
    "        is_in_boxes_all = is_in_boxes.sum(dim=0) > 0  # 计算总共的在真实框内数目\n",
    "        # in fixed center\n",
    "\n",
    "        center_radius = 2.5\n",
    "\n",
    "        gt_bboxes_per_image_l = (gt_bboxes_per_image[:, 0]).unsqueeze(1).repeat(\n",
    "            1, total_num_anchors\n",
    "        ) - center_radius * expanded_strides_per_image.unsqueeze(0)\n",
    "        gt_bboxes_per_image_r = (gt_bboxes_per_image[:, 0]).unsqueeze(1).repeat(\n",
    "            1, total_num_anchors\n",
    "        ) + center_radius * expanded_strides_per_image.unsqueeze(0)\n",
    "        gt_bboxes_per_image_t = (gt_bboxes_per_image[:, 1]).unsqueeze(1).repeat(\n",
    "            1, total_num_anchors\n",
    "        ) - center_radius * expanded_strides_per_image.unsqueeze(0)\n",
    "        gt_bboxes_per_image_b = (gt_bboxes_per_image[:, 1]).unsqueeze(1).repeat(\n",
    "            1, total_num_anchors\n",
    "        ) + center_radius * expanded_strides_per_image.unsqueeze(0)\n",
    "\n",
    "        c_l = x_centers_per_image - gt_bboxes_per_image_l\n",
    "        c_r = gt_bboxes_per_image_r - x_centers_per_image\n",
    "        c_t = y_centers_per_image - gt_bboxes_per_image_t\n",
    "        c_b = gt_bboxes_per_image_b - y_centers_per_image\n",
    "        center_deltas = torch.stack([c_l, c_t, c_r, c_b], 2)\n",
    "        is_in_centers = center_deltas.min(dim=-1).values > 0.0\n",
    "        is_in_centers_all = is_in_centers.sum(dim=0) > 0\n",
    "\n",
    "        # in boxes and in centers\n",
    "        is_in_boxes_anchor = is_in_boxes_all | is_in_centers_all\n",
    "\n",
    "        is_in_boxes_and_center = (\n",
    "                is_in_boxes[:, is_in_boxes_anchor] & is_in_centers[:, is_in_boxes_anchor]\n",
    "        )\n",
    "        return is_in_boxes_anchor, is_in_boxes_and_center\n",
    "\n",
    "    def dynamic_k_matching(self, cost, pair_wise_ious, gt_classes, num_gt, fg_mask):\n",
    "        # Dynamic K\n",
    "        # ---------------------------------------------------------------\n",
    "        matching_matrix = torch.zeros_like(cost, dtype=torch.uint8)\n",
    "\n",
    "        ious_in_boxes_matrix = pair_wise_ious\n",
    "        n_candidate_k = min(10, ious_in_boxes_matrix.size(1))\n",
    "        topk_ious, _ = torch.topk(ious_in_boxes_matrix, n_candidate_k, dim=1)\n",
    "        dynamic_ks = torch.clamp(topk_ious.sum(1).int(), min=1)\n",
    "        dynamic_ks = dynamic_ks.tolist()\n",
    "        for gt_idx in range(num_gt):\n",
    "            _, pos_idx = torch.topk(\n",
    "                cost[gt_idx], k=dynamic_ks[gt_idx], largest=False\n",
    "            )\n",
    "            matching_matrix[gt_idx][pos_idx] = 1\n",
    "\n",
    "        del topk_ious, dynamic_ks, pos_idx\n",
    "\n",
    "        anchor_matching_gt = matching_matrix.sum(0)\n",
    "        if (anchor_matching_gt > 1).sum() > 0:\n",
    "            _, cost_argmin = torch.min(cost[:, anchor_matching_gt > 1], dim=0)\n",
    "            matching_matrix[:, anchor_matching_gt > 1] *= 0\n",
    "            matching_matrix[cost_argmin, anchor_matching_gt > 1] = 1\n",
    "        fg_mask_inboxes = matching_matrix.sum(0) > 0\n",
    "        num_fg = fg_mask_inboxes.sum().item()\n",
    "\n",
    "        fg_mask[fg_mask.clone()] = fg_mask_inboxes\n",
    "\n",
    "        matched_gt_inds = matching_matrix[:, fg_mask_inboxes].argmax(0)\n",
    "        gt_matched_classes = gt_classes[matched_gt_inds]\n",
    "\n",
    "        pred_ious_this_matching = (matching_matrix * pair_wise_ious).sum(0)[\n",
    "            fg_mask_inboxes\n",
    "        ]\n",
    "        return num_fg, gt_matched_classes, pred_ious_this_matching, matched_gt_inds\n",
    "\n",
    "\n",
    "class YOLOX(nn.Module):\n",
    "    def __init__(self, backbone=YOLOPAFPN(), head=YOLOXHead(num_classes=4)):\n",
    "        super(YOLOX, self).__init__()\n",
    "        if backbone is None:\n",
    "            backbone = YOLOPAFPN()\n",
    "        if head is None:\n",
    "            head = YOLOXHead(num_classes=4)\n",
    "\n",
    "        self.backbone = backbone()\n",
    "        self.head = head(num_classes=4)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        fpn_outs = self.backbone(x)\n",
    "\n",
    "        if self.training:\n",
    "            assert targets is not None\n",
    "            loss, iou_loss, conf_loss, cls_loss, l1_loss, num_fg = self.head(\n",
    "                fpn_outs, targets, x\n",
    "            )\n",
    "            outputs = {\n",
    "                \"total_loss\": loss,\n",
    "                \"iou_loss\": iou_loss,\n",
    "                \"l1_loss\": l1_loss,\n",
    "                \"conf_loss\": conf_loss,\n",
    "                \"cls_loss\": cls_loss,\n",
    "                \"num_fg\": num_fg,\n",
    "            }\n",
    "        else:\n",
    "            outputs = self.head(fpn_outs)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86dcd5b",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "用于测试：\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "648e701b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-22T11:24:17.532644Z",
     "start_time": "2022-02-22T11:24:17.517696Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19664/2454824235.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mYOLOX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dog.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19664/3138173782.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, backbone, head)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackbone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 738\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    740\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    from cv2 import cv2\n",
    "    import numpy as np\n",
    "\n",
    "    net = YOLOX()\n",
    "    img = cv2.imread(\"dog.jpg\")\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    img = torch.from_numpy(img).to(torch.float32)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "    img = net(img)\n",
    "    print(img)\n",
    "#     img = torch.squeeze(img, 0)\n",
    "#     img = img.detach().numpy()\n",
    "#     img = img * 255 / img.max()\n",
    "#     img = np.transpose(img, (1, 2, 0))\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) #无需更改通道\n",
    "\n",
    "#     cv2.imshow(\"test\", img)\n",
    "#     cv2.waitKey()\n",
    "    # print(img[\"dark3\"].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
